ğŸ‘‰ ################ Pre-Requisite of Cluster Upgrade?  ################

â¦	Cordon the nodes [ Make the node un schedulable ]
â¦	Backup critical data [ kubectl get all --all-namespaces -o yaml > cluster-backup.yaml ] 
â¦	Make sure we go through the Release notes [ 1.30 to 1.31 ] - Understand what changed between versions (ex: v1.30 â†’ v1.31).
â¦	We need to make sure We upgraded successfully in lower environments , Verify  everything is working in lower environment 
â¦	Please note K8s cluster upgrade is irreversible, meaning we cannot downgrade . So Ensure we test upgrade in lower environment and make sure we touch production environment after 2-3 weeks 
â¦	Control plane of Kubernetes and Worker nodes should be on same version 
â¦	If we are using clusterAutoscaler, Make sure clusterAutoscaler version matches with the Control plane
â¦	At least 5 available IP Address are available in your subnet where cluster resides 
â¦	Kubelet that runs on all nodes , its version should also match the Control plane Version

ğŸ‘‰ ################ Process of Cluster Upgrade?  ################

â¦	Upgrade Managed Control Plane - Note updates are not done automatically - Done  through [ UI / ekctl / CLI ]. We call it Managed Control Plane, because AWS maintains High Availability of Control plane, keeping control plane in different regions, takes care of  security, Scaling up our API server when requests increases. Thats why we call managed Control plane 

# eksctl upgrade cluster --name cluster_name --region us-east-1  --version 1.31
# az aks upgrade   --resource-group <rg>   --name <cluster-name>   --kubernetes-version <target-version>   --control-plane-only

â¦	Upgrade the node groups using the rollout process. If not using a node group, then we should upgrade the nodes 
# Update using Rolling Update [ Both launch Templates and custom AMI ] 
# az aks nodepool upgrade   --resource-group <rg>   --cluster-name <cluster-name>   --name <nodepool-name>   --kubernetes-version <target-version>


â¦	Upgrade the Add-Ons / Plugins 

ğŸ‘‰ ################ How to Test the Upgrade? ################
Write a simple script, or run the  regression / functional   Test on the Upgraded Cluster. 

a. Verify versions [ kubectl get nodes ] 
b. Verify the kube-system pods [ kubectl get pods -n kube-system ] 
c. Validate applications
Logs & Metrics
Ingress/services
Autoscaling behavior

--> Pod health & readiness checks
--> Autoscaling tests
--> Logs & metrics validation
--> Ingress & load balancing checks
--> Version verification for all add-ons

ğŸ‘‰ ################  Why can't we downgrade the cluster if something goes wrong? ################ 
Because Kubernetes doesnâ€™t support downgrading.
Upgrades remove old API versions, changes the etcd schema, and modify controller behaviour.
ğ—” ğ—±ğ—¼ğ˜„ğ—»ğ—´ğ—¿ğ—®ğ—±ğ—² ğ˜„ğ—¼ğ˜‚ğ—¹ğ—± ğ—¯ğ—¿ğ—²ğ—®ğ—¸:
--> API servers
--> CRDs
--> DNS
--> Networking
--> Controllers
So the only safe rollback is:
Create a new cluster on the old version â†’ migrate workloads.

ğŸ‘‰ ################ ImagePullBackoff Issue ################ 
When a kubelet starts creating containers for a Pod using a container runtime, it might be possible the container is in Waiting state because of ImagePullBackOff.
The status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image for reasons such as

Invalid image name or
Pulling from a private registry without imagePullSecret.

# kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>
where:
<your-registry-server> is your Private Docker Registry FQDN. Use https://index.docker.io/v1/ for DockerHub.
<your-name> is your Docker username.
<your-pword> is your Docker password.
<your-email> is your Docker email.

# Create a Pod that uses your Secret
---
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred

ğŸ‘‰ ################ CrashLoopBackOff Issue - ################  

When you see "CrashLoopBackOff," it means that kubelet is trying to run the container, but it keeps failing and crashing. After crashing, Kubernetes tries to restart the container automatically, but if the container keeps failing repeatedly, you end up in a loop of crashes and restarts, thus the term "CrashLoopBackOff."

Misconfigurations - if wrong env variable or typo in env variable
Wrong Command Line Arguments -  when wrong command or argument is passed in the manifest 
The Memory Limits Are Too Low -  if there is resource request / limit issue leading to OOM  
Errors in the Liveness Probes - Liveness probes in Kubernetes are used to check the health of a container. If a liveness probe is incorrectly configured, it might falsely report that the container is unhealthy,

ğŸ‘‰ ################ pods-not-schedulable Issue - ################ 
Node selector , Taints and toleration, Node Affinity , Check the scheduler 

ğŸ‘‰ ################ A pod keeps Crashing , Why ? ################ 
Check logs of pod # k logs pod_name 
Check pod events  #k describe po pod_name 
Check for resource limit
Check how many times it restarted  # k get po -o wide 

ğŸ‘‰ ################  A Service is not reaching the correct pod ? ################ 
Check for labels and selectors 
Check if the pod is healthy 
Check if it's due to network policy 
Check the pod port and target port of the  service,and  see if something is wrongly set up 


ğŸ‘‰ ################ What is a PodDisruptionBudget (PDB)? ################ 

A PodDisruptionBudget limits how many pods of an application can be voluntarily disrupted at the same time.
Voluntary disruptions include:

Node drains (for maintenance or upgrades)
Cluster autoscaling (nodes being removed)
Manual pod evictions

You define one of these:

minAvailable: minimum pods that must stay running
maxUnavailable: maximum pods that can be unavailable

ğŸ‘‰ ################ ğğ¨ğ ğ’ğ­ğ®ğœğ¤ ğ¢ğ§ ğğğ§ğğ¢ğ§ğ  ################

If a Kubernetes Pod is stuck in Pending, it means the pod was accepted by the cluster but cannot be scheduled or started. Below is a practical, step-by-step troubleshooting checklist used in real clusters.

1. Check pod events [ kubectl describe pod <pod-name>] 
Common messages youâ€™ll see:

Insufficient cpu
Insufficient memory
node(s) had taint
no nodes available
failed to pull image
persistentvolumeclaim is not bound

2. Check node availability & status
Ensure:
Nodes are Ready
Not NotReady, SchedulingDisabled

3. Resource requests too high
4. PersistentVolumeClaim (PVC) issues
5. Node selectors / affinity rules
6. Taints and tolerations
7. Image pull issues (sometimes Pending)
8. Namespace resource quotas
9. Scheduler issues (rare)

ğŸ‘‰ ################  ğğ¨ğğ ğ¢ğ§ ğğ¨ğ­ğ‘ğğšğğ² ################ 
Kubelet down
Disk pressure
Network issues
Fix:
Restart kubelet
Free disk space
Fix node-control plane connectivity

1ï¸âƒ£ Check node status & reason
kubectl get nodes
kubectl describe node <node-name>
Look for:
Ready=False

Conditions like:
NetworkUnavailable
KubeletNotReady
ContainerRuntimeNotReady

2ï¸âƒ£ SSH into the node
3ï¸âƒ£ Check kubelet service

Common problems:
kubelet stopped
cert expired
cannot connect to API server
disk pressure

4ï¸âƒ£ Check container runtime (Docker / containerd)
sudo systemctl status containerd
sudo journalctl -u containerd -n 50
sudo systemctl restart containerd

5ï¸âƒ£ Disk, memory, or inode pressure
6ï¸âƒ£ Network / CNI issues
If CNI plugin fails, node wonâ€™t be Ready.
kubectl logs -n kube-system <cni-pod>
8ï¸âƒ£ Certificates expired
sudo kubeadm certs check-expiration
sudo kubeadm certs renew all
sudo systemctl restart kubelet
9ï¸âƒ£ API server connectivity
Node must reach API server on port 6443.
sudo systemctl restart containerd kubelet

ğŸ‘‰ ################ ğˆğ§ğ ğ«ğğ¬ğ¬ ğğ¨ğ­ ğ–ğ¨ğ«ğ¤ğ¢ğ§ğ   ################ 
If Kubernetes Ingress is not working, the issue is usually controller, service, DNS, or rules mismatch.
1ï¸âƒ£ Confirm Ingress Controller exists
kubectl get pods -n ingress-nginx
2ï¸âƒ£ Check Ingress resource
Correct HOST
Correct SERVICE NAME
Correct SERVICE PORT
No errors in Events
3ï¸âƒ£ IngressClass mismatch
4ï¸âƒ£ Service behind Ingress
5ï¸âƒ£ Pods behind Service are healthy
8ï¸âƒ£ TLS / HTTPS problems
Check:
Secret exists
Secret type is kubernetes.io/tls
9ï¸âƒ£ Check ingress controller logs
kubectl logs -n ingress-nginx <controller-pod>

#### RedHat Interview ######
RedHat CKA Questions - 

Explain k8s architecture
Explain the role of kube proxy and kubelet 

there are 3 master nodes , one master node is brought down , kubectl is not working , Why ? 

what will happen if kubelet is not running? news pods will not run , what about the existing containers? will the be running ?

Do you know , split etcd ? 

Explain node Selector 

lets say we have 2 replicas, 1 pod is running with no issue, the other when trying to place on worker node, its throwing crashbacklookup issue 
There are 5 replicas , one pod is not running. How will you stop the traffic to unhealthy pod in order to troubleshoot? 


 
Difference b/n git merge and git rebase 
How do rollback the last comit in git ?

https://www.linkedin.com/in/ay-garg/
https://www.linkedin.com/in/apurvanisal/
